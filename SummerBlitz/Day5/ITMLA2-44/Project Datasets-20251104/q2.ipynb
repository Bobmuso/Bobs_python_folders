{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13aaede5",
   "metadata": {},
   "source": [
    "2.1. Using the features selected from Question 1 (from RFE and SelectKBest), implement the following two machine learning\n",
    "algorithms for predicting dementia using the results from both feature selection methods for comparison:\n",
    "\n",
    "a) Logistic Regression\n",
    "(10 marks)\n",
    "\n",
    "b) Decision tree\n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d0bad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: missing variables in kernel: ['X_train', 'X_test', 'y_train', 'y_test']\n",
      "Created synthetic dataset for demonstration. Define your real X_train/X_test/y_train/y_test to use actual data.\n",
      "SelectKBest: n_features=10  Accuracy=0.417  AUC=0.370\n",
      "RFE: n_features=10  Accuracy=0.417  AUC=0.370\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import os\n",
    "\n",
    "def load_feature_list(fname, fallback_cols):\n",
    "    if os.path.exists(fname):\n",
    "        return pd.read_csv(fname, header=None).iloc[:,0].astype(str).tolist()\n",
    "    return fallback_cols\n",
    "\n",
    "fallback = list(X_train.columns[:10])\n",
    "skb_selected = globals().get(\"skb_selected\", None)\n",
    "rfe_selected = globals().get(\"rfe_selected\", None)\n",
    "\n",
    "if skb_selected is None:\n",
    "    skb_selected = load_feature_list(\"selectkbest_selected_features.csv\", fallback)\n",
    "if rfe_selected is None:\n",
    "    rfe_selected = load_feature_list(\"rfecv_selected_features.csv\", fallback)\n",
    "\n",
    "models = {\"SelectKBest\": skb_selected, \"RFE\": rfe_selected}\n",
    "\n",
    "for name, feats in models.items():\n",
    "    Xtr = X_train[feats]\n",
    "    Xte = X_test[feats]\n",
    "    clf = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
    "    clf.fit(Xtr, y_train)\n",
    "    preds = clf.predict(Xte)\n",
    "    probs = clf.predict_proba(Xte)[:,1] if hasattr(clf, \"predict_proba\") else clf.decision_function(Xte)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    auc = roc_auc_score(y_test, probs)\n",
    "    print(f\"{name}: n_features={len(feats)}  Accuracy={acc:.3f}  AUC={auc:.3f}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea70a9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import os, pandas as pd\n",
    "\n",
    "fallback = list(X_train.columns[:10])\n",
    "skb_selected = globals().get(\"skb_selected\", None)\n",
    "rfe_selected = globals().get(\"rfe_selected\", None)\n",
    "def load_feature_list(fname, fallback_cols):\n",
    "    if os.path.exists(fname):\n",
    "        return pd.read_csv(fname, header=None).iloc[:,0].astype(str).tolist()\n",
    "    return fallback_cols\n",
    "if skb_selected is None:\n",
    "    skb_selected = load_feature_list(\"selectkbest_selected_features.csv\", fallback)\n",
    "if rfe_selected is None:\n",
    "    rfe_selected = load_feature_list(\"rfecv_selected_features.csv\", fallback)\n",
    "\n",
    "for name, feats in {\"SelectKBest\": skb_selected, \"RFE\": rfe_selected}.items():\n",
    "    Xtr = X_train[feats]\n",
    "    Xte = X_test[feats]\n",
    "    clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "    clf.fit(Xtr, y_train)\n",
    "    preds = clf.predict(Xte)\n",
    "    probs = clf.predict_proba(Xte)[:,1]\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    auc = roc_auc_score(y_test, probs)\n",
    "    print(f\"{name}: n_features={len(feats)}  Accuracy={acc:.3f}  AUC={auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649ebd87",
   "metadata": {},
   "source": [
    "2.2. Evaluate and compare the performance of each algorithm you implemented in 2.1. above. Justify your choice of evaluation\n",
    "metrics used and provide a summary of which algorithm performed better for both feature selection methods, and why you think\n",
    "that is the case.\n",
    "(15 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672bcf5",
   "metadata": {},
   "source": [
    "Metrics used: AUC (primary) and Accuracy, Precision, Recall, F1 reported. AUC is preferred because it measures ranking performance and is robust to class imbalance; accuracy alone can be misleading.\n",
    "\n",
    "Summary: the cell above prints metrics and then reports which algorithm had the higher AUC for each feature-selection method; prefer the model with higher AUC for overall discrimination, and use precision/recall/F1 to check clinical trade-offs (e.g., recall if missing dementia is costly)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
