{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5184f1",
   "metadata": {},
   "source": [
    "Diagnosing Patients with Dementia\n",
    "\n",
    "You are a data scientist working for a healthcare organisation that aims to improve early detection of Dementia. The organisation\n",
    "has provided you with a rich dataset containing health information for 2,149 patients. Each patient is uniquely identified by a\n",
    "Patient ID, and the dataset contains a variety of features such as demographic details, lifestyle factors, medical history, clinical\n",
    "measurements, cognitive assessments, and symptoms.\n",
    "\n",
    "Your task is to build a machine learning model to predict whether a patient has Dementia based on the available data. This\n",
    "classification model will help doctors identify high-risk patients and prioritise them for further diagnostic tests or interventions.\n",
    "\n",
    "The dataset is named \"dementia.csv\" and can be downloaded from the “Project Datasets” folder on myLMS.\n",
    "\n",
    "Appendix A has a detailed description of all the columns within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63abe3dc",
   "metadata": {},
   "source": [
    "1.1.\n",
    "Performing the necessary preprocessing on the data to get it ready for training the machine learning model. This will\n",
    "include removing missing values, encoding categorical variables, feature standardisation and any necessary feature engineering.\n",
    "(5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d96af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done. Shapes: (1362, 35) (341, 35) (1362,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_103221/4137568578.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace({\"None\": np.nan, \"XXXConfid\": np.nan}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load csv data\n",
    "df = pd.read_csv(\"dementia.csv\", index_col=0)\n",
    "\n",
    "# 'None'/'XXXConfid' as missing\n",
    "df.replace({\"None\": np.nan, \"XXXConfid\": np.nan}, inplace=True)\n",
    "for c in (\"PatientID\", \"DoctorInCharge\"):\n",
    "    if c in df.columns:\n",
    "        df.drop(columns=c, inplace=True)\n",
    "\n",
    "# drop rows with any missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# target and features\n",
    "# Diagnosis may be stored as a boolean, strings, or numeric\n",
    "diag = df[\"Diagnosis\"]\n",
    "\n",
    "if diag.dtype == object:\n",
    "    # try map strings to ints\n",
    "    y = diag.map({\"Yes\": 1, \"No\": 0})\n",
    "    # if mapping left any NA (e.g. some values are numeric strings), try coercing to numeric\n",
    "    if y.isnull().any():\n",
    "        y = pd.to_numeric(diag, errors=\"coerce\")\n",
    "else:\n",
    "    # numeric already (or boolean)\n",
    "    y = pd.to_numeric(diag, errors=\"coerce\")\n",
    "\n",
    "# no missing values remain in the target\n",
    "if y.isnull().any():\n",
    "    raise ValueError(\"Diagnosis column contains values that cannot be converted to a binary target. \"\n",
    "                     \"Inspect df['Diagnosis'] for unexpected values.\")\n",
    "\n",
    "y = y.astype(int)\n",
    "X = df.drop(columns=[\"Diagnosis\"])\n",
    "\n",
    "# encode categoricals with one-hot\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# standardise numeric features\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "# train/test split for downstream work\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Preprocessing done. Shapes:\", X_train.shape, X_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd51b540",
   "metadata": {},
   "source": [
    "1.2.\n",
    "Implement both of the following approaches for feature selection:\n",
    "\n",
    "a) A filter-based method using Select KBest\n",
    "\n",
    "b) A wrapper-based method using Recursive Feature Elimination (RFE) with a suitable estimator\n",
    "\n",
    "Your implementation should clearly show the selected features in each case.\n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bddbb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest (k=10) selected 10 features:\n",
      " - HeadInjury\n",
      " - Hypertension\n",
      " - CholesterolLDL\n",
      " - CholesterolHDL\n",
      " - MMSE\n",
      " - FunctionalAssessment\n",
      " - MemoryComplaints\n",
      " - BehavioralProblems\n",
      " - ADL\n",
      " - Gender_Male\n",
      "\n",
      "RFE (n_features=10) selected 10 features:\n",
      " - CholesterolLDL\n",
      " - MMSE\n",
      " - FunctionalAssessment\n",
      " - MemoryComplaints\n",
      " - BehavioralProblems\n",
      " - ADL\n",
      " - Ethnicity_Other\n",
      " - EducationLevel_Higher\n",
      " - Smoking_Yes\n",
      " - Forgetfulness_Yes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# use preprocessed train split\n",
    "X = X_train.copy()\n",
    "y = y_train.copy().squeeze()\n",
    "\n",
    "# choose a small k for simplicity\n",
    "k = min(10, X.shape[1])\n",
    "\n",
    "# a) Filter: SelectKBest\n",
    "skb = SelectKBest(score_func=f_classif, k=k).fit(X, y)\n",
    "skb_selected = list(X.columns[skb.get_support()])\n",
    "print(f\"SelectKBest (k={k}) selected {len(skb_selected)} features:\")\n",
    "for f in skb_selected:\n",
    "    print(\" -\", f)\n",
    "\n",
    "# b) Wrapper: RFE \n",
    "est = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
    "rfe = RFE(estimator=est, n_features_to_select=k).fit(X, y)\n",
    "rfe_selected = list(X.columns[rfe.support_])\n",
    "print(f\"\\nRFE (n_features={k}) selected {len(rfe_selected)} features:\")\n",
    "for f in rfe_selected:\n",
    "    print(\" -\", f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7dbd5c",
   "metadata": {},
   "source": [
    "1.3.\n",
    "For both RFE and SelectKBest:\n",
    "\n",
    "-Discuss the advantages and limitations of the methods\n",
    "(5 marks)\n",
    "\n",
    "-Explain the process for selecting the optimal number of features for each approach \n",
    "(5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c464e92",
   "metadata": {},
   "source": [
    "SelectKBest — advantages\n",
    "\n",
    "Fast, simple and model‑agnostic (uses univariate scores).\n",
    "Easy to interpret and reproducible.\n",
    "SelectKBest — limitations\n",
    "\n",
    "Ignores feature interactions and multicollinearity.\n",
    "May select redundant features with high univariate score.\n",
    "RFE (wrapper) — advantages\n",
    "\n",
    "Considers features in combination and how they affect model performance.\n",
    "Tends to produce feature sets tailored to the chosen estimator.\n",
    "\n",
    "RFE — limitations\n",
    "\n",
    "Computationally expensive for many features.\n",
    "Model‑dependent (selection may not transfer to other estimators) and can overfit if not validated.\n",
    "How to choose k for SelectKBest (short)\n",
    "\n",
    "Sweep k (e.g. grid of values) and evaluate downstream CV metric (accuracy/AUC) on X_train.\n",
    "Plot score vs k and pick the k at maximum or where performance plateaus (or use domain constraints).\n",
    "Use nested or held‑out CV to avoid optimistic selection.\n",
    "\n",
    "How to choose number of features for RFE (short)\n",
    "\n",
    "Use RFECV (RFE with CV) to pick optimal n_features automatically via cross‑validation.\n",
    "Alternatively run RFE with a grid of n_features and evaluate CV performance; choose the smallest feature set with near‑maximal CV score (1‑SE rule).\n",
    "Always validate final selection on an independent test set to check generalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d9ce25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest: n_features=10  Accuracy=0.880  AUC=0.940\n",
      "RFE: n_features=10  Accuracy=0.865  AUC=0.943\n"
     ]
    }
   ],
   "source": [
    "# question 2 A\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import os\n",
    "\n",
    "def load_feature_list(fname, fallback_cols):\n",
    "    if os.path.exists(fname):\n",
    "        return pd.read_csv(fname, header=None).iloc[:,0].astype(str).tolist()\n",
    "    return fallback_cols\n",
    "\n",
    "fallback = list(X_train.columns[:10])\n",
    "skb_selected = globals().get(\"skb_selected\", None)\n",
    "rfe_selected = globals().get(\"rfe_selected\", None)\n",
    "\n",
    "if skb_selected is None:\n",
    "    skb_selected = load_feature_list(\"selectkbest_selected_features.csv\", fallback)\n",
    "if rfe_selected is None:\n",
    "    rfe_selected = load_feature_list(\"rfecv_selected_features.csv\", fallback)\n",
    "\n",
    "models = {\"SelectKBest\": skb_selected, \"RFE\": rfe_selected}\n",
    "\n",
    "for name, feats in models.items():\n",
    "    Xtr = X_train[feats]\n",
    "    Xte = X_test[feats]\n",
    "    clf = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
    "    clf.fit(Xtr, y_train)\n",
    "    preds = clf.predict(Xte)\n",
    "    probs = clf.predict_proba(Xte)[:,1] if hasattr(clf, \"predict_proba\") else clf.decision_function(Xte)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    auc = roc_auc_score(y_test, probs)\n",
    "    print(f\"{name}: n_features={len(feats)}  Accuracy={acc:.3f}  AUC={auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest: n_features=10  Accuracy=0.947  AUC=0.949\n",
      "RFE: n_features=10  Accuracy=0.935  AUC=0.931\n"
     ]
    }
   ],
   "source": [
    "# question 2 B\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import os, pandas as pd\n",
    "\n",
    "\n",
    "fallback = list(X_train.columns[:10])\n",
    "skb_selected = globals().get(\"skb_selected\", None)\n",
    "rfe_selected = globals().get(\"rfe_selected\", None)\n",
    "def load_feature_list(fname, fallback_cols):\n",
    "    if os.path.exists(fname):\n",
    "        return pd.read_csv(fname, header=None).iloc[:,0].astype(str).tolist()\n",
    "    return fallback_cols\n",
    "if skb_selected is None:\n",
    "    skb_selected = load_feature_list(\"selectkbest_selected_features.csv\", fallback)\n",
    "if rfe_selected is None:\n",
    "    rfe_selected = load_feature_list(\"rfecv_selected_features.csv\", fallback)\n",
    "\n",
    "for name, feats in {\"SelectKBest\": skb_selected, \"RFE\": rfe_selected}.items():\n",
    "    Xtr = X_train[feats]\n",
    "    Xte = X_test[feats]\n",
    "    clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "    clf.fit(Xtr, y_train)\n",
    "    preds = clf.predict(Xte)\n",
    "    probs = clf.predict_proba(Xte)[:,1]\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    auc = roc_auc_score(y_test, probs)\n",
    "    print(f\"{name}: n_features={len(feats)}  Accuracy={acc:.3f}  AUC={auc:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
